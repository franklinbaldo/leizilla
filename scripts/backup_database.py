import logging
import os
from pathlib import Path
from datetime import datetime

from src.publisher import InternetArchivePublisher
from src.config import DUCKDB_PATH, IA_ACCESS_KEY, IA_SECRET_KEY
from src.storage import storage as duckdb_storage # Import global storage instance

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration for database backup
DB_FILE_PATH = DUCKDB_PATH
IA_DB_COLLECTION = "leizilla-database-backups"
IA_DB_IDENTIFIER_PREFIX = "leizilla-duckdb-backup"

def backup_duckdb_to_ia():
    """
    Backs up the DuckDB database file to the Internet Archive.
    """
    logger.info(f"Starting DuckDB backup process for {DB_FILE_PATH}...")

    if not IA_ACCESS_KEY or not IA_SECRET_KEY:
        logger.error("Internet Archive API keys (IA_ACCESS_KEY, IA_SECRET_KEY) are not set.")
        logger.error("Cannot proceed with database backup.")
        return

    if not DB_FILE_PATH.exists():
        logger.error(f"Database file not found at {DB_FILE_PATH}. Cannot proceed with backup.")
        return

    publisher = InternetArchivePublisher()

    # Create a timestamped identifier for the backup
    timestamp = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
    db_identifier = f"{IA_DB_IDENTIFIER_PREFIX}-{timestamp}"

    # Basic metadata for the database backup
    # The publisher's `_generate_ia_identifier` is more for individual laws.
    # For a backup, a simpler identifier generation is fine, and we pass it directly.
    # We'll construct a simplified metadata dictionary for `_prepare_ia_metadata`.
    backup_metadata_for_publisher = {
        'titulo': f"Leizilla DuckDB Backup {timestamp}",
        'origem': "leizilla-system",
        'tipo_lei': "database-backup", # Custom type
        'ano': datetime.now().year,
        'numero': timestamp, # Using timestamp as a unique part
        'metadados': {
            'original_filename': DB_FILE_PATH.name,
            'backup_timestamp': datetime.now().isoformat(),
            'backup_type': 'duckdb_database'
        }
    }

    # The `upload_pdf` method is generic enough if we treat the db file as a "pdf"
    # It primarily needs a file path and metadata.
    # We can adapt its internal metadata preparation or rely on its flexibility.
    # Let's ensure the identifier is passed correctly for backups.
    # The `_generate_ia_identifier` in publisher might not be suitable for this type of generic backup.
    # So, we will use a custom identifier and pass it.

    # We need to use a slightly different approach than `upload_pdf` if we want to control the identifier directly
    # and use a different collection. Let's adapt parts of the publisher logic or make it more flexible.
    # For now, we can use a workaround:
    # The `upload_pdf` method uses `_generate_ia_identifier` internally.
    # To override this for backups, we'd ideally have a more generic `upload_file` method in the publisher.
    # As a workaround, we can call the private `_execute_ia_upload` if necessary,
    # or ensure `_prepare_ia_metadata` and `_generate_ia_identifier` can handle this case.

    # Let's try to use the existing `upload_pdf` by ensuring the metadata fits its expectations,
    # and specify the collection. The identifier will be generated by the publisher.
    # We can add a 'backup_tag' to distinguish it if needed.

    logger.info(f"Preparing to upload {DB_FILE_PATH.name} to Internet Archive.")
    logger.info(f"Target IA Collection: {IA_DB_COLLECTION}")
    # The identifier will be generated by `publisher._generate_ia_identifier` based on metadata
    # This might look like: leizilla-leizilla-system-database-backup-2023-backup-yyyymmddhhmmss

    upload_result = publisher.upload_pdf(
        pdf_path=DB_FILE_PATH, # Treating .duckdb file as the file to upload
        lei_metadata=backup_metadata_for_publisher,
        collection=IA_DB_COLLECTION
    )

    if upload_result.get('success'):
        logger.info(f"Successfully uploaded database backup {DB_FILE_PATH.name} to IA.")
        logger.info(f"IA Detail URL: {upload_result.get('ia_detail_url')}")
        logger.info(f"IA File URL: {upload_result.get('ia_pdf_url')}") # This will be the .duckdb file URL
    else:
        logger.error(f"Failed to upload database backup {DB_FILE_PATH.name} to IA.")
        logger.error(f"Error: {upload_result.get('error')}")

if __name__ == "__main__":
    # Note: DuckDB should ideally be closed or checkpointed before backup
    # for maximum safety, especially if there are concurrent writes.
    # For a GitHub Action that runs sequentially, this might be less of an issue
    # as the DB operations from the crawler script should be finished.
    # However, explicitly closing connections or checkpointing would be best practice.
    # For now, we assume the DB file is in a consistent state.

    try:
        logger.info(f"Attempting to connect to DuckDB at {DB_FILE_PATH} and perform checkpoint.")
        conn = duckdb_storage.connect()
        conn.execute("CHECKPOINT;")
        logger.info("DuckDB CHECKPOINT command executed.")
        duckdb_storage.close() # Closes the connection and flushes WAL
        logger.info("DuckDB connection closed, WAL flushed.")
    except Exception as e:
        logger.warning(f"Could not perform DuckDB checkpoint or close connection: {e}. Proceeding with backup anyway.")

    backup_duckdb_to_ia()
